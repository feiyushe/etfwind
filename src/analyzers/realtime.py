"""ç®€åŒ–ç‰ˆæŠ•èµ„åˆ†æ - æ— æ•°æ®åº“ï¼Œå®æ—¶åˆ†æ"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from collections import Counter
from loguru import logger
import httpx

from src.config import settings
from src.models import NewsItem
from src.collectors import NewsAggregator


# å…¨å±€ç¼“å­˜
_cache = {
    "result": None,
    "updated_at": None,
    "news_count": 0,
    "source_stats": {},  # å„æ¥æºé‡‡é›†ç»Ÿè®¡
}

# å®šæ—¶ä»»åŠ¡æ§åˆ¶
_scheduler_task = None

ANALYSIS_PROMPT = """ä½ æ˜¯Aè‚¡ETFæŠ•èµ„åˆ†æå¸ˆï¼Œç”¨å°çº¢ä¹¦é£æ ¼è¾“å‡ºæŠ•èµ„å‚è€ƒï¼ˆé€‚å½“ä½¿ç”¨emojiè®©å†…å®¹æ›´ç”ŸåŠ¨ï¼‰ã€‚

## æ–°é—»ï¼ˆå…±{count}æ¡ï¼‰
{news_list}

## è¾“å‡ºè¦æ±‚
```json
{{
  "market_view": "ğŸ¯ å½“å‰å¸‚åœºçŠ¶æ€ä¸€å¥è¯æ€»ç»“ï¼ˆ20å­—å†…ï¼Œå¸¦emojiï¼‰",
  "narrative": "å¸‚åœºå…¨æ™¯åˆ†æï¼ˆ150å­—ï¼ŒåŒ…å«ä¸»è¦çŸ›ç›¾ã€æƒ…ç»ªã€è¶‹åŠ¿ï¼Œé€‚å½“åŠ emojiï¼‰",
  "sectors": [
    {{
      "name": "èŠ¯ç‰‡",
      "direction": "åˆ©å¥½",
      "reason": "ğŸ“ˆ æ¶¨ä»·+çŸ­ç¼º",
      "events": [
        {{"title": "ğŸ”¥ ç¾å…‰æ¶¨5%", "suggestion": "ğŸ’¡ å¯å…³æ³¨"}}
      ]
    }}
  ],
  "risk_level": "ä¸­"
}}
```

æ³¨æ„ï¼š
- sectors æœ€å¤š6ä¸ªï¼ŒæŒ‰é‡è¦æ€§æ’åº
- name å¿…é¡»æ˜¯æ ‡å‡†æ¿å—åï¼šèŠ¯ç‰‡/åŠå¯¼ä½“/äººå·¥æ™ºèƒ½/é€šä¿¡/æœºå™¨äºº/å…‰ä¼/æ–°èƒ½æº/æ–°èƒ½æºè½¦/é”‚ç”µæ± /å†›å·¥/åŒ»è¯/åˆ›æ–°è¯/è¯åˆ¸/é“¶è¡Œ/æˆ¿åœ°äº§/ç™½é…’/æ¶ˆè´¹/å†œä¸š/é»„é‡‘/æœ‰è‰²/ç…¤ç‚­/é’¢é“/çŸ³æ²¹/æ’ç”Ÿç§‘æŠ€/æ¸¯è‚¡/æ¸¸æˆ/ä¼ åª’/ç”µåŠ›
- æ¯ä¸ª sector åŒ…å« events æ•°ç»„ï¼ˆ1-2ä¸ªç›¸å…³äº‹ä»¶ï¼‰ï¼Œäº‹ä»¶ title å‰åŠ emoji
- direction: åˆ©å¥½/åˆ©ç©º/ä¸­æ€§
- reason å‰åŠ åˆé€‚emojiï¼ˆğŸ“ˆğŸ“‰âš ï¸ğŸ’°ğŸ”¥ï¼‰
- suggestion å‰åŠ ğŸ’¡ï¼Œ15å­—å†…
- ä¸šç»©é¢„å‘Šè¦èšåˆçœ‹è¡Œä¸šè¶‹åŠ¿
- risk_level: ä½/ä¸­/é«˜
"""


async def collect_news() -> tuple[list[NewsItem], dict]:
    """é‡‡é›†æ‰€æœ‰æºçš„æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, æ¥æºç»Ÿè®¡)"""
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        # ç»Ÿè®¡å„æ¥æºæ•°é‡
        stats = Counter(item.source for item in news.items)
        return news.items, dict(stats)
    finally:
        await agg.close()


async def analyze(items: list[NewsItem]) -> dict:
    """AIåˆ†ææ–°é—»"""
    base_url = settings.claude_base_url.rstrip("/")
    api_key = settings.claude_api_key
    model = settings.claude_model

    news_list = "\n".join([
        f"{i+1}. [{item.source}] {item.title}"
        for i, item in enumerate(items)
    ])

    prompt = ANALYSIS_PROMPT.format(count=len(items), news_list=news_list)

    try:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{base_url}/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                },
                json={
                    "model": model,
                    "max_tokens": 4096,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"].strip()

        # æå– JSON
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]

        return json.loads(text)
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
        return {}


async def refresh() -> dict:
    """åˆ·æ–°åˆ†æç»“æœ"""
    global _cache

    logger.info("å¼€å§‹é‡‡é›†æ–°é—»...")
    items, source_stats = await collect_news()
    logger.info(f"é‡‡é›†åˆ° {len(items)} æ¡æ–°é—»: {source_stats}")

    logger.info("å¼€å§‹AIåˆ†æ...")
    result = await analyze(items)

    beijing_tz = timezone(timedelta(hours=8))
    _cache = {
        "result": result,
        "updated_at": datetime.now(beijing_tz),
        "news_count": len(items),
        "source_stats": source_stats,
    }

    logger.info("åˆ†æå®Œæˆ")
    return result


def get_cache() -> dict:
    """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    return _cache


async def get_or_refresh(max_age_minutes: int = 60) -> dict:
    """è·å–ç»“æœï¼Œè¿‡æœŸåˆ™åˆ·æ–°"""
    global _cache

    if _cache["result"] is None:
        return await refresh()

    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    age = now - _cache["updated_at"]

    if age.total_seconds() > max_age_minutes * 60:
        return await refresh()

    return _cache["result"]


async def _scheduler_loop(interval_minutes: int = 30):
    """å®šæ—¶åˆ·æ–°å¾ªç¯"""
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            logger.info(f"å®šæ—¶åˆ·æ–°å¼€å§‹ (é—´éš” {interval_minutes} åˆ†é’Ÿ)")
            await refresh()
        except asyncio.CancelledError:
            logger.info("å®šæ—¶ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"å®šæ—¶åˆ·æ–°å¤±è´¥: {e}")


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task is None:
        _scheduler_task = asyncio.create_task(_scheduler_loop(interval_minutes))
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
        logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
